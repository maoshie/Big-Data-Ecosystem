
* AULA 8 SQOOP - Exercicios - Sqoop – Importação BD Employees - Otimizacao *

# Realizar com uso do MySQL

# acessar mysql: docker exec -it database bash
mysql -psecret

-----------------------------------------------------------------------------
1. Criar a tabela cp_titles_date, contendo a cópia da tabela titles 
com os campos title e to_date.

database: employees;
table origen: titles;
new_tables: cp_titles_data 
columns: title, to_date


# TABELA titles:
mysql> select * from titles limit 2;
+--------+-----------------+------------+------------+
| emp_no | title           | from_date  | to_date    |
+--------+-----------------+------------+------------+
|  10001 | Senior Engineer | 1986-06-26 | 9999-01-01 |
|  10002 | Staff           | 1996-08-03 | 9999-01-01 |
+--------+-----------------+------------+------------+

# NOVA TABELA cp_titles_date:

mysql> create table cp_titles_data select title, to_date from titles;
Query OK, 443308 rows affected (3.74 sec)
Records: 443308  Duplicates: 0  Warnings: 0

mysql> select * from cp_titles_data limit 10;
+-----------------+------------+
| title           | to_date    |
+-----------------+------------+
| Senior Engineer | 9999-01-01 |
| Staff           | 9999-01-01 |
| Senior Engineer | 9999-01-01 |
| Engineer        | 1995-12-01 |
| Senior Engineer | 9999-01-01 |
| Senior Staff    | 9999-01-01 |
| Staff           | 1996-09-12 |
| Senior Engineer | 9999-01-01 |
| Senior Staff    | 9999-01-01 |
| Staff           | 1996-02-11 |
+-----------------+------------+
10 rows in set (0.00 sec)


-----------------------------------------------------------------------------
2. Pesquisar os 15 primeiros registros da tabela cp_titles_date.

mysql> select * from cp_titles_data limit 15;
+--------------------+------------+
| title              | to_date    |
+--------------------+------------+
| Senior Engineer    | 9999-01-01 |
| Staff              | 9999-01-01 |
| Senior Engineer    | 9999-01-01 |
| Engineer           | 1995-12-01 |
| Senior Engineer    | 9999-01-01 |
| Senior Staff       | 9999-01-01 |
| Staff              | 1996-09-12 |
| Senior Engineer    | 9999-01-01 |
| Senior Staff       | 9999-01-01 |
| Staff              | 1996-02-11 |
| Assistant Engineer | 2000-07-31 |
| Assistant Engineer | 1990-02-18 |
| Engineer           | 1995-02-18 |
| Senior Engineer    | 9999-01-01 |
| Engineer           | 9999-01-01 |
+--------------------+------------+
15 rows in set (0.00 sec)


-----------------------------------------------------------------------------
3. Alterar os registros do campo to_date para null da tabela cp_titles_date, 
quando o título for igual a Staff.

valor: NULL
campo: title = staff

mysql> update cp_titles_data set to_date = NUll where title = 'Staff';

mysql> select * from cp_titles_data limit 10;
+-----------------+------------+
| title           | to_date    |
+-----------------+------------+
| Senior Engineer | 9999-01-01 |
| Staff           | NULL       |
| Senior Engineer | 9999-01-01 |
| Engineer        | 1995-12-01 |
| Senior Engineer | 9999-01-01 |
| Senior Staff    | 9999-01-01 |
| Staff           | NULL       |
| Senior Engineer | 9999-01-01 |
| Senior Staff    | 9999-01-01 |
| Staff           | NULL       |
+-----------------+------------+
10 rows in set (0.00 sec)


-----------------------------------------------------------------------------

# Realizar com uso do Sqoop - Importações no warehouse 
/user/hive/warehouse/db_test_<numero_questao> e visualizar no HDFS

4. Importar a tabela titles com 8 mapeadores no formato parquet

table: titles;
mapeadores: -m 8;
formato: --as-parquetfile;

# comando:
sqoop import --table titles --connect jdbc:mysql://database/employees
--username root --password secret -m 8 --as-parquetfile  
--warehouse-dir /user/hive/warehouse/db_test_4

root@namenode:/# hdfs dfs -ls -R /user/hive/warehouse/db_test_4
/user/hive/warehouse/db_test_4/titles/06b8ac99-13ab-4740-a62c-4cf6ab189585.parquet
/user/hive/warehouse/db_test_4/titles/2237bb38-267c-451d-8b32-190df6d6ce56.parquet
/user/hive/warehouse/db_test_4/titles/77c72153-6681-4650-a079-c353a3084041.parquet
/user/hive/warehouse/db_test_4/titles/cae37bb2-00b0-4ce6-bc88-06379487ab8c.parquet
/user/hive/warehouse/db_test_4/titles/d38cc720-68e1-46a1-91af-587b71e85fc1.parquet
/user/hive/warehouse/db_test_4/titles/fb021e4d-99ed-448e-8a31-0978cd6b181c.parquet


-----------------------------------------------------------------------------
5. Importar a tabela titles com 8 mapeadores no formato parquet e compressão snappy

table: titles;
mapeadores: -m 8;
formato: --as-parquetfile;
compression: SNAPPY; org.apache.hadoop.io.compress.SnappyCode
  #1 - Habilitar compressão: --compress
  #2 - Escolher compressão: --compression-codec <compression>

#COMANDO:
sqoop import --table titles --connect jdbc:mysql://database/employees 
--username root --password secret -m 8 --as-parquetfile 
--warehouse-dir /user/hive/warehouse/db_test_5 
--compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec


# VISUALIZAÇÃO:
root@namenode:/# hdfs dfs -du -h /user/hive/warehouse/db_test_5/
3.1 M  /user/hive/warehouse/db_test_5/titles


-----------------------------------------------------------------------------
6. Importar a tabela cp_titles_date com 4 mapeadores (erro)

# UTILIZAR: --split-by title


---------------------------------------------------------------------------------
A. Importar a tabela cp_titles_data com 4 mapeadores divididos pelo campo título 
no warehouse /user/hive/warehouse/db_test2_title

# table: cp_titles_data;
# UTILIZAR: --split-by title;
# mapeadores: -m 4;
#caminho: /user/hive/warehouse/db_test2_title

+-----------------+------------+
| title           | to_date    |
+-----------------+------------+
| Senior Engineer | 9999-01-01 |

sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true
--table cp_titles_data --connect jdbc:mysql://database/employees
--username root --password secret -m 4 
--warehouse-dir /user/hive/warehouse/db_test2_title --split-by title

# VISUALIZANDO:
root@namenode:/# hdfs dfs -du -h /user/hive/warehouse/db_test2_title
8.8 M  /user/hive/warehouse/db_test2_title/cp_titles_data

-------------------------------------------------------------------------------
B. Importar a tabela cp_titles_date com 4 mapeadores divididos pelo campo data 
no warehouse /user/hive/warehouse/db_test2_date

+-----------------+------------+
| title           | to_date    |
+-----------------+------------+
| Senior Engineer | 9999-01-01 |

# table: cp_titles_data;
# UTILIZAR: --split-by to_date;
# mapeadores: -m 4;
#caminho: /user/hive/warehouse/db_test2_date

sqoop import --table cp_titles_data --connect jdbc:mysql://database/employees
--username root --password secret -m 4 
--warehouse-dir /user/hive/warehouse/db_test2_date --split-by to_date

root@namenode:/# hdfs dfs -ls -R /user/hive/warehouse/db_test2_date/
/user/hive/warehouse/db_test2_date/cp_titles_data
/user/hive/warehouse/db_test2_date/cp_titles_data/_SUCCESS
/user/hive/warehouse/db_test2_date/cp_titles_data/part-m-00000
/user/hive/warehouse/db_test2_date/cp_titles_data/part-m-00001
/user/hive/warehouse/db_test2_date/cp_titles_data/part-m-00002
/user/hive/warehouse/db_test2_date/cp_titles_data/part-m-00003



C. Qual a diferença dos registros nulos entre as duas importações?

# DIFERENÇA ENTRE AS DUAS:

# --split-by to_date;
root@namenode:/# hdfs dfs -du -h /user/hive/warehouse/db_test2_date
7.7 M  /user/hive/warehouse/db_test2_date/cp_titles_data


# --split-by title;
root@namenode:/# hdfs dfs -du -h /user/hive/warehouse/db_test2_title/
8.8 M  /user/hive/warehouse/db_test2_title/cp_titles_data
root@namenode:/#

-----------------------------------------------------------------------------